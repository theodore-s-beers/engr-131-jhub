{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from pykubegrader.tokens.validate_token import validate_token\n",
    "validate_token('type the key provided by your instructor here', assignment = 'week7-quiz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "validate_token(assignment = 'week7-quiz')\n",
    "\n",
    "# You must make sure to run all cells in sequence using shift + enter or you might encounter errors\n",
    "from pykubegrader.initialize import initialize_assignment\n",
    "\n",
    "responses = initialize_assignment(\"1_quiz_q\", \"week_7\", \"quiz\", assignment_points = 28.0, assignment_tag = 'week7-quiz')\n",
    "\n",
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"1_quiz_q.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# ❓Quiz: Object-Oriented Programming in Piping Fluid Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Run this block of code by pressing Shift + Enter to display the question\n",
    "from questions._1_quiz_q import Question1\n",
    "Question1().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Run this block of code by pressing Shift + Enter to display the question\n",
    "from questions._1_quiz_q import Question2\n",
    "Question2().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Run this block of code by pressing Shift + Enter to display the question\n",
    "from questions._1_quiz_q import Question3\n",
    "Question3().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1 (Points: 12.0): Modify a Pipeline Class\n",
    "\n",
    "Given the `Pipeline` class below, modify it to:\n",
    "- Add an `__init__()` method that initializes the `name` and `flow_rate` attributes. The `flow_rate` attribute should default to 0.\n",
    "- Add an `increase_flow(rate)` method that increases the `flow_rate` attribute. \n",
    "- Add a `decrease_flow(rate)` method that decreases `flow_rate` but ensures it doesn’t go below zero.\n",
    "- Implement a `__str__()` method that returns `\"Pipeline: <name>, Flow Rate: <flow_rate> L/s\"`.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "pipe = Pipeline(\"Main\", 50)\n",
    "pipe.increase_flow(20)\n",
    "print(pipe)  # Output: Pipeline: Main, Flow Rate: 70 L/s\n",
    "pipe.decrease_flow(80)\n",
    "print(pipe)  # Output: Pipeline: Main, Flow Rate: 0 L/s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "pipe = Pipeline(\"Main\", 50)\n",
    "pipe.increase_flow(20)\n",
    "print(pipe)  # Output: Pipeline: Main, Flow Rate: 70 L/s\n",
    "pipe.decrease_flow(80)\n",
    "print(pipe)  # Output: Pipeline: Main, Flow Rate: 0 L/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "grader.check(\"Free-Response-Modify-a-Pipeline-Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Submitting Assignment\n",
    "\n",
    "Please run the following block of code using `shift + enter` to submit your assignment, you should see your score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from pykubegrader.tokens.validate_token import validate_token\n",
    "validate_token(assignment = 'week7-quiz')\n",
    "\n",
    "\n",
    "from pykubegrader.submit.submit_assignment import submit_assignment\n",
    "\n",
    "submit_assignment(\"week7-quiz\", \"1_quiz_q\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "Free-Response-Modify-a-Pipeline-Class": {
     "name": "Free-Response-Modify-a-Pipeline-Class",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> import base64\n>>> from pykubegrader.tokens.validate_token import validate_token\n>>> validate_token(assignment='week7-quiz')\n>>> max_question_points = str(12.0)\n>>> earned_points = 0\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> os.environ['TOTAL_POINTS_FREE_RESPONSE'] = str(12.0)\n>>> from pykubegrader.tokens.validate_token import validate_token\n>>> validate_token(assignment='week7-quiz')\n>>> log_variable('total-points', f'week7-quiz, 1_quiz_q', 12.0)\n>>> question_id = 'Free-Response-Modify-a-Pipeline-Class-1'\n>>> max_score = 2.0\n>>> score = 0\n>>> item = Pipeline('Main', 50)\n>>> pipeline_initialization = item.name == 'Main' and item.flow_rate == 50\n>>> assert pipeline_initialization, f\"Expected Pipeline('Main', 50), but got name={item.name}, flow_rate={item.flow_rate}.\"\n>>> if pipeline_initialization:\n...     score = 2.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(pipeline_initialization))\n",
         "failure_message": "Failed: Pipeline object does not initialize correctly.",
         "hidden": false,
         "locked": false,
         "points": 2,
         "success_message": "Success: Pipeline object initializes correctly!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> import base64\n>>> from pykubegrader.tokens.validate_token import validate_token\n>>> validate_token(assignment='week7-quiz')\n>>> question_id = 'Free-Response-Modify-a-Pipeline-Class-2'\n>>> max_score = 2.0\n>>> score = 0\n>>> item = Pipeline('Main')\n>>> default_flow_rate_check = item.flow_rate == 0\n>>> assert default_flow_rate_check, f'Expected default flow_rate to be 0, but got {item.flow_rate}.'\n>>> if default_flow_rate_check:\n...     score = 2.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(default_flow_rate_check))\n",
         "failure_message": "Failed: Default flow_rate value is incorrect.",
         "hidden": false,
         "locked": false,
         "points": 2,
         "success_message": "Success: Default stock value is correctly set to 0!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> import base64\n>>> from pykubegrader.tokens.validate_token import validate_token\n>>> validate_token(assignment='week7-quiz')\n>>> question_id = 'Free-Response-Modify-a-Pipeline-Class-3'\n>>> max_score = 2.0\n>>> score = 0\n>>> item = Pipeline('Main', 10)\n>>> item.increase_flow(20)\n>>> flow_after_addition = item.flow_rate\n>>> assert flow_after_addition == 30, f'Expected flow_rate to be 30 after addition, but got {item.flow_rate}.'\n>>> if flow_after_addition == 30:\n...     score = 2.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(flow_after_addition))\n",
         "failure_message": "Failed: `increase_flow()` does not increase flow_rate correctly.",
         "hidden": false,
         "locked": false,
         "points": 2,
         "success_message": "Success: `increase_flow()` correctly increases flow_rate!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> import base64\n>>> from pykubegrader.tokens.validate_token import validate_token\n>>> validate_token(assignment='week7-quiz')\n>>> question_id = 'Free-Response-Modify-a-Pipeline-Class-4'\n>>> max_score = 2.0\n>>> score = 0\n>>> item = Pipeline('Main', 50)\n>>> item.decrease_flow(30)\n>>> flow_after_removal = item.flow_rate\n>>> assert flow_after_removal == 20, f'Expected flow_rate to be 20 after removal, but got {item.flow_rate}.'\n>>> if flow_after_removal == 20:\n...     score = 2.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(flow_after_removal))\n",
         "failure_message": "Failed: `decrease_flow()` does not decrease flow_rate correctly.",
         "hidden": false,
         "locked": false,
         "points": 2,
         "success_message": "Success: `decrease_flow()` correctly decreases flow_rate!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> import base64\n>>> from pykubegrader.tokens.validate_token import validate_token\n>>> validate_token(assignment='week7-quiz')\n>>> question_id = 'Free-Response-Modify-a-Pipeline-Class-5'\n>>> max_score = 2.0\n>>> score = 0\n>>> item = Pipeline('Main', 40)\n>>> item.decrease_flow(100)\n>>> flow_after_excessive_removal = item.flow_rate\n>>> assert flow_after_excessive_removal == 0, f'Expected flow_rate to be 0 (not negative), but got {item.flow_rate}.'\n>>> if flow_after_excessive_removal == 0:\n...     score = 2.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(flow_after_excessive_removal))\n",
         "failure_message": "Failed: Flow rate goes below 0.",
         "hidden": false,
         "locked": false,
         "points": 2,
         "success_message": "Success: Flow rate does not drop below 0!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> import base64\n>>> from pykubegrader.tokens.validate_token import validate_token\n>>> validate_token(assignment='week7-quiz')\n>>> question_id = 'Free-Response-Modify-a-Pipeline-Class-6'\n>>> max_score = 2.0\n>>> score = 0\n>>> item = Pipeline('Main', 15)\n>>> string_representation = str(item)\n>>> expected_string = 'Pipeline: Main, Flow Rate: 15 L/s'\n>>> assert string_representation == expected_string, f\"Expected '{expected_string}', but got '{string_representation}'.\"\n>>> if string_representation == expected_string:\n...     score = 2.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(string_representation))\n",
         "failure_message": "Failed: `__str__()` does not return the expected format.",
         "hidden": false,
         "locked": false,
         "points": 2,
         "success_message": "Success: `__str__()` returns the correct format!"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
