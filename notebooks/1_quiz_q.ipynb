{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e38044f",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "from pykubegrader.tokens.validate_token import validate_token\n",
        "validate_token('type the key provided by your instructor here') \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48f84bc",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "# You must make sure to run all cells in sequence using shift + enter or you might encounter errors\n",
        "from pykubegrader.initialize import initialize_assignment\n",
        "\n",
        "responses = initialize_assignment(\"1_quiz_q\", \"week_4\", \"quiz\", assignment_points = 19.0, assignment_tag = 'week4-quiz')\n",
        "\n",
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook(\"1_quiz_q.ipynb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "source": [
        "# ❓ Quiz: Control Structures for Engineering Applications\n",
        "\n",
        "Test your understanding of conditional statements (`if-else`) and loops in Python by solving problems relevant to engineering applications, such as sensor data processing, control systems, and automation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd479b0d",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "# Run this block of code by pressing Shift + Enter to display the question\n",
        "from questions._1_quiz_q import Question1\n",
        "Question1().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd48e8b9",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "# Run this block of code by pressing Shift + Enter to display the question\n",
        "from questions._1_quiz_q import Question2\n",
        "Question2().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ca239fa",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "# Run this block of code by pressing Shift + Enter to display the question\n",
        "from questions._1_quiz_q import Question3\n",
        "Question3().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "deletable": false,
        "editable": false
      },
      "source": [
        "## Water Turbidity Monitoring System\n",
        "\n",
        "Water turbidity is a measure of water clarity, often used to ensure safe drinking water or proper water quality in engineering applications. Turbidity is measured in Nephelometric Turbidity Units (NTU). In this task, you will implement a program to process turbidity data collected over time and check for compliance with safety standards.\n",
        "\n",
        "The program should:\n",
        "\n",
        "1. Iterate through a list of turbidity measurements (in NTU).\n",
        "2. Print a warning message if the turbidity exceeds the safe threshold of 5.0 NTU.\n",
        "3. Calculate the average turbidity of all measurements and display it.\n",
        "4. Determine if the average turbidity complies with the safe threshold (≤ 5.0 NTU) and display a summary message.\n",
        "\n",
        "### Implementation Details\n",
        "\n",
        "1. Define a list of turbidity measurements: `turbidity_readings = [2.5, 3.0, 5.5, 6.2, 4.8, 3.3]`.\n",
        "2. Set the safe turbidity threshold: `safe_threshold = 5.0`.\n",
        "3. Loop through all turbidity measurements and:\n",
        "   - Print a warning message if the turbidity exceeds the safe threshold.\n",
        "   - The loop should assign the local variable `turbidity` in each iteration of the for loop. \n",
        "4. Calculate the average turbidity of all measurements. Save to the variable `average_turbidity`.\n",
        "   - Print the average turbidity value. We have provided the code for you to calculate the average turbidity. You can use the `sum()` and `len()` functions to calculate the average, or the numpy library if you prefer.\n",
        "5. Check if the average turbidity complies with the safe threshold:\n",
        "   - If the average turbidity is less than or equal to the safe threshold, print a message indicating compliance.\n",
        "   - If the average turbidity exceeds the safe threshold, print a message indicating non-compliance.\n",
        "   We have provided the print statements for you. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18",
      "metadata": {
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "# Define the turbidity measurements\n",
        "...\n",
        "\n",
        "# Define the safe turbidity threshold\n",
        "...\n",
        "\n",
        "def turbidity_warning(turbidity, threshold):\n",
        "\n",
        "    # Iterate through turbidity readings\n",
        "    ...\n",
        "        \n",
        "        # Check if the turbidity exceeds the safe threshold\n",
        "        ...\n",
        "            print(f\"Warning: Turbidity exceeds safe threshold at {turbidity} NTU!\")\n",
        "\n",
        "    # Calculate the average turbidity\n",
        "    ...\n",
        "\n",
        "    # Print the average turbidity\n",
        "    print(f\"Average turbidity: {average_turbidity:.2f} NTU\")\n",
        "\n",
        "    # Check if the average turbidity is safe\n",
        "    ...\n",
        "        print(\"The average turbidity is within the safe range.\")\n",
        "    ...\n",
        "        print(\"The average turbidity exceeds the safe threshold!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dbd1882",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "grader.check(\"Water-Turbidity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3caa42d9",
      "metadata": {},
      "source": [
        "## Submitting Assignment\n",
        "\n",
        "Please run the following block of code using `shift + enter` to submit your assignment, you should see your score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8084e2d2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "from pykubegrader.tokens.validate_token import validate_token\n",
        "validate_token()\n",
        "\n",
        "\n",
        "from pykubegrader.submit.submit_assignment import submit_assignment\n",
        "\n",
        "submit_assignment(\"week4-quiz\", \"1_quiz_q\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "engr131_dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "otter": {
      "OK_FORMAT": true,
      "tests": {
        "Water-Turbidity": {
          "name": "Water-Turbidity",
          "points": null,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> max_question_points = str(10.0)\n>>> earned_points = 0\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> os.environ['TOTAL_POINTS_FREE_RESPONSE'] = str(10.0)\n>>> from pykubegrader.tokens.validate_token import validate_token\n>>> validate_token()\n>>> log_variable('total-points', f'Reading-Week-X, 1_quiz_q', 10.0)\n>>> question_id = 'Water-Turbidity-1'\n>>> max_score = 1.0\n>>> score = 0\n>>> condition = isinstance(turbidity_readings, list) and len(turbidity_readings) > 0 and all((isinstance(t, (int, float)) for t in turbidity_readings))\n>>> assert condition, 'Turbidity readings must be a list of numerical values.'\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(turbidity_readings))\n",
                  "failure_message": "Failed: Turbidity measurements are not defined correctly.",
                  "hidden": false,
                  "locked": false,
                  "points": 1,
                  "success_message": "Success: Turbidity measurements are defined correctly!"
                },
                {
                  "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Water-Turbidity-2'\n>>> max_score = 1.0\n>>> score = 0\n>>> condition = isinstance(safe_threshold, (int, float)) and safe_threshold > 0\n>>> assert condition, 'Safe threshold must be a positive numerical value.'\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(safe_threshold))\n",
                  "failure_message": "Failed: The safe turbidity threshold is not defined correctly.",
                  "hidden": false,
                  "locked": false,
                  "points": 1,
                  "success_message": "Success: The safe turbidity threshold is defined correctly!"
                },
                {
                  "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Water-Turbidity-3'\n>>> max_score = 1.0\n>>> score = 0\n>>> iteration_count = 0\n>>> for turbidity in turbidity_readings:\n...     iteration_count += 1\n>>> expected_count = len(turbidity_readings)\n>>> condition = iteration_count == expected_count\n>>> assert condition, f'Expected iteration over {expected_count} readings, but iterated {iteration_count} times.'\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(iteration_count))\n>>> responses = update_responses(question_id, str(expected_count))\n",
                  "failure_message": "Failed: The code does not correctly iterate through turbidity readings.",
                  "hidden": false,
                  "locked": false,
                  "points": 1,
                  "success_message": "Success: The code correctly iterates through turbidity readings!"
                },
                {
                  "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Water-Turbidity-4'\n>>> max_score = 2.0\n>>> score = 0\n>>> high_turbidity = [turbidity for turbidity in turbidity_readings if turbidity > safe_threshold]\n>>> expected_high_turbidity = [5.5, 6.2]\n>>> condition = high_turbidity == expected_high_turbidity\n>>> assert condition, f'Expected warnings for {expected_high_turbidity}, but got warnings for {high_turbidity}.'\n>>> if condition:\n...     score = 2.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(high_turbidity))\n>>> responses = update_responses(question_id, str(expected_high_turbidity))\n",
                  "failure_message": "Failed: The function does not identify or warn about high turbidity readings.",
                  "hidden": false,
                  "locked": false,
                  "points": 2,
                  "success_message": "Success: The function correctly identifies and warns about high turbidity readings!"
                },
                {
                  "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Water-Turbidity-5'\n>>> max_score = 2.0\n>>> score = 0\n>>> expected_average = sum(turbidity_readings) / len(turbidity_readings)\n>>> average_turbidity = sum(turbidity_readings) / len(turbidity_readings)\n>>> condition = abs(average_turbidity - expected_average) < 1e-06\n>>> assert condition, f'Expected average: {expected_average}, but got: {average_turbidity}.'\n>>> if condition:\n...     score = 2.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(average_turbidity))\n>>> responses = update_responses(question_id, str(expected_average))\n",
                  "failure_message": "Failed: The function does not correctly calculate the average turbidity.",
                  "hidden": false,
                  "locked": false,
                  "points": 2,
                  "success_message": "Success: The function correctly calculates the average turbidity!"
                },
                {
                  "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> import re\n>>> from io import StringIO\n>>> import sys\n>>> question_id = 'Water-Turbidity-6'\n>>> max_score = 3.0\n>>> score = 0\n>>> output = StringIO()\n>>> sys.stdout = output\n>>> turbidity_readings = [2.5, 3.0, 5.5, 6.2, 4.8, 3.3]\n>>> safe_threshold = 5.0\n>>> expected_outputs = ['Warning: Turbidity exceeds safe threshold at 5.5 NTU!', 'Warning: Turbidity exceeds safe threshold at 6.2 NTU!', 'Average turbidity: 4.22 NTU', 'The average turbidity is within the safe range.']\n>>> turbidity_warning(turbidity_readings, safe_threshold)\n>>> sys.stdout = sys.__stdout__\n>>> output_value = output.getvalue().strip()\n>>> condition = all((re.search(re.escape(expected), output_value, re.IGNORECASE) for expected in expected_outputs))\n>>> assert condition, f'Expected outputs: {expected_outputs}, but got: {output_value}'\n>>> if condition:\n...     score = 3.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('1_quiz_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n",
                  "failure_message": "Failed: The function does not handle all cases correctly for the given input.",
                  "hidden": false,
                  "locked": false,
                  "points": 3,
                  "success_message": "Success: The function handles all cases correctly for the given input!"
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
